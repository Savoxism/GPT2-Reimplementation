1) In GPT2, the positional embeddings are learnable, not fixed like the original transformers 

2) For GPT2, layernorms are applied before attention and ffn

3) Multi-head attention allows the model to capture different aspects of the input sequence by having multiple attention heads, each learning different representations. Each attention head processes a hs-dimensional subspace of the full embedding independently, and their results are later concatenated back into a C-dimensional vector.

4) When we calculate the loss, the label is just the token to our right.

5) param sharing. In the og paper attention is all you need, author mentioned "In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation."

6) code for moving to GPU
device = "cpu"
if torch.cuda.is_available():
    device = "cuda"
elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
    device = "mps"
print("using device:", device)

7) Code for training, the simplest form
model = GPT(GPTConfig())
model.to(device)

optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)
for i in range(50):
    optimizer.zero_grad()
    logits, loss = model(x, y)
    loss.backward()
    optimizer.step()
    print(f"step {i}, loss: {loss.item()}")

8) code for fetching the NEXT BATCH of training data ->>> TRAIN_DATALOADER
def next_batch(self):
        B, T = self.B, self.T
        buf = self.tokens[self.current_position:self.current_position + B*T + 1] # extract a contiguous chunk of tokens from self.tokens, starting at self.current_position
        x = (buf[:-1]).view(B, T)
        y = (buf[1:]).view(B, T)
        # advance the position in the tensor
        self.current_position += B * T
        # if loading the next batch would be out of bound, reset
        if self.current_position + B * T + 1 > len(self.tokens):
            self.current_position = 0
        return x, y

9) Sometimes, the loss that we gained was from basically deleting the usage of tokens that never occur, probably most of the loss gain -> false realization

10) weight initialization for linear layers, embeddings, and residual paths
std *= (2 * self.config.n_layer) ** -0.5 2 layers that form a residual path computation, from attention and from mlp 
-> avoid bias

11) Different floating point precisions representations: FP16 and BF16, mostly used for AI large scale training, INT8 is used for inference. The fewer bits of representations, the easier to move them around -> the bandwidth is equally very important, sometimes the computations are extremely fast but the data fetching from memory is not fast enough.

12) Understand Tensor Cores (matrix decomposition), torch.autocast -> faster

13) for the smallest gpt2 model 124M, the largest matrix mul is from the top classifer layer

14) mixed precision, conversion from fp16 to bf16 incurs a minimal enhancement, gradient scalers (complicating) -> trade off fast & precise

15) torch.compile() == gcc for c++ -> super useful and powerful, kernel fusion, Python Overhead -> much faster

16) flash attention: a kernel fusion operation -> faster

17) nice / ugly numbers => Simple yet so effective

18) hyperparam tuning, gradient clipping, learning rate scheduler 

19) batch size scheduler (useful in later stages of training), weight decay, fusedadamW 
Fused Adam is a high-performance, optimized implementation of the Adam optimizer that combines multiple operations into a single fused kernel for faster and more memory-efficient execution on GPUs.
dont apply weight decay on 1-dimensional tensors 

20) gradient accumulation => enable large batch size with limited GPU processing power  ===>> accum = sum in loss

21) distributed data parallel -> stride out the procceses in the train dataloader, understand ddp rank local rank, world size

22) Training loop before ddp
optimizer = model.configure_optimizers(weight_decay=0.01, learning_rate=6e-4, device=device)

for step in range(max_steps):
    t0 = time.time()
    optimizer.zero_grad()
    loss_accum = 0.0
    for micro_step in range(grad_accum_steps):
        x, y = train_loader.next_batch()
        x, y = x.to(device), y.to(device)
        with torch.autocast(device_type = device, dtype=torch.bfloat16):
            logits, loss = model(x, y)
        # scale the loss to account for gradient accumulation, because we want to average the gradients, not sum them
        loss = loss / grad_accum_steps
        loss_accum += loss.detach()
        loss.backward()
    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
    # determine learning rate
    lr = get_lr(step)
    for param_group in optimizer.param_groups:
        param_group['lr'] = lr    
    optimizer.step()
    # torch.cuda.synchronize() # disable if cuda is not available
    t1 = time.time()
    dt = (t1 - t0) * 1000 
    tokens_processed = train_loader.B * train_loader.T * grad_accum_steps
    tokens_per_sec = tokens_processed / dt
    print(f"step {step:4d} | lr: {lr:.4e} | loss: {loss_accum.item():.6f} | norm: {norm} | time: {dt:.2f}ms | tok/sec: {tokens_per_sec:.2f}")

23) GPT2 is trained on Reddit 

24) ✅ Downloads the FineWeb-Edu dataset.
✅ Tokenizes text using GPT-2 tokenizer (tiktoken).
✅ Processes data in parallel using multiprocessing.
✅ Splits tokenized data into .npy shards (each 100M tokens).
✅ Saves the first shard as "val" (validation set), the rest as "train".

25

26

27











