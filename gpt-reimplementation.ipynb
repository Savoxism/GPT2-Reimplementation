{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-21T13:43:25.358800Z","iopub.execute_input":"2024-10-21T13:43:25.359297Z","iopub.status.idle":"2024-10-21T13:43:25.724100Z","shell.execute_reply.started":"2024-10-21T13:43:25.359249Z","shell.execute_reply":"2024-10-21T13:43:25.723283Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt","metadata":{"execution":{"iopub.status.busy":"2024-10-21T13:43:25.725794Z","iopub.execute_input":"2024-10-21T13:43:25.726190Z","iopub.status.idle":"2024-10-21T13:43:26.989891Z","shell.execute_reply.started":"2024-10-21T13:43:25.726156Z","shell.execute_reply":"2024-10-21T13:43:26.988777Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"--2024-10-21 13:43:26--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1115394 (1.1M) [text/plain]\nSaving to: 'input.txt'\n\ninput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.04s   \n\n2024-10-21 13:43:26 (28.5 MB/s) - 'input.txt' saved [1115394/1115394]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"with open(\"input.txt\", 'r', encoding='utf-8') as f:\n    text = f.read()","metadata":{"execution":{"iopub.status.busy":"2024-10-21T13:43:26.991656Z","iopub.execute_input":"2024-10-21T13:43:26.992612Z","iopub.status.idle":"2024-10-21T13:43:26.999534Z","shell.execute_reply.started":"2024-10-21T13:43:26.992557Z","shell.execute_reply":"2024-10-21T13:43:26.998307Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"print(len(text))","metadata":{"execution":{"iopub.status.busy":"2024-10-21T13:43:27.001718Z","iopub.execute_input":"2024-10-21T13:43:27.002097Z","iopub.status.idle":"2024-10-21T13:43:27.008763Z","shell.execute_reply.started":"2024-10-21T13:43:27.002064Z","shell.execute_reply":"2024-10-21T13:43:27.007756Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"1115394\n","output_type":"stream"}]},{"cell_type":"code","source":"print(text[:1000])","metadata":{"execution":{"iopub.status.busy":"2024-10-21T13:43:27.009762Z","iopub.execute_input":"2024-10-21T13:43:27.010017Z","iopub.status.idle":"2024-10-21T13:43:27.020122Z","shell.execute_reply.started":"2024-10-21T13:43:27.009989Z","shell.execute_reply":"2024-10-21T13:43:27.019226Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"First Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou are all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you know Caius Marcius is chief enemy to the people.\n\nAll:\nWe know't, we know't.\n\nFirst Citizen:\nLet us kill him, and we'll have corn at our own price.\nIs't a verdict?\n\nAll:\nNo more talking on't; let it be done: away, away!\n\nSecond Citizen:\nOne word, good citizens.\n\nFirst Citizen:\nWe are accounted poor citizens, the patricians good.\nWhat authority surfeits on would relieve us: if they\nwould yield us but the superfluity, while it were\nwholesome, we might guess they relieved us humanely;\nbut they think we are too dear: the leanness that\nafflicts us, the object of our misery, is as an\ninventory to particularise their abundance; our\nsufferance is a gain to them Let us revenge this with\nour pikes, ere we become rakes: for the gods know I\nspeak this in hunger for bread, not in thirst for revenge.\n\n\n","output_type":"stream"}]},{"cell_type":"code","source":"chars = sorted(list(set(text)))\nvocab_size = len(chars)\nprint(''.join(chars))\nprint(vocab_size)","metadata":{"execution":{"iopub.status.busy":"2024-10-21T13:43:27.021549Z","iopub.execute_input":"2024-10-21T13:43:27.021831Z","iopub.status.idle":"2024-10-21T13:43:27.045361Z","shell.execute_reply.started":"2024-10-21T13:43:27.021802Z","shell.execute_reply":"2024-10-21T13:43:27.044390Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"\n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n65\n","output_type":"stream"}]},{"cell_type":"code","source":"# create a mapping from characters to integers\nstoi = { ch:i for i,ch in enumerate(chars) }\nitos = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n\nprint(encode(\"hii there\"))\nprint(decode(encode(\"hii there\")))","metadata":{"execution":{"iopub.status.busy":"2024-10-21T13:43:27.046383Z","iopub.execute_input":"2024-10-21T13:43:27.047317Z","iopub.status.idle":"2024-10-21T13:43:27.055784Z","shell.execute_reply.started":"2024-10-21T13:43:27.047271Z","shell.execute_reply":"2024-10-21T13:43:27.054844Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"[46, 47, 47, 1, 58, 46, 43, 56, 43]\nhii there\n","output_type":"stream"}]},{"cell_type":"code","source":"# let's now encode the entire text dataset and store it into a torch.Tensor\nimport torch # we use PyTorch: https://pytorch.org\ndata = torch.tensor(encode(text), dtype=torch.long)\nprint(data.shape, data.dtype)\nprint(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this","metadata":{"execution":{"iopub.status.busy":"2024-10-21T13:43:27.056793Z","iopub.execute_input":"2024-10-21T13:43:27.057218Z","iopub.status.idle":"2024-10-21T13:43:30.535478Z","shell.execute_reply.started":"2024-10-21T13:43:27.057185Z","shell.execute_reply":"2024-10-21T13:43:30.534493Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"torch.Size([1115394]) torch.int64\ntensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n","output_type":"stream"}]},{"cell_type":"code","source":"# Let's now split up the data into train and validation sets\nn = int(0.9*len(data)) # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]","metadata":{"execution":{"iopub.status.busy":"2024-10-21T13:43:30.537229Z","iopub.execute_input":"2024-10-21T13:43:30.537798Z","iopub.status.idle":"2024-10-21T13:43:30.542844Z","shell.execute_reply.started":"2024-10-21T13:43:30.537748Z","shell.execute_reply":"2024-10-21T13:43:30.541738Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"block_size = 8\ntrain_data[:block_size+1]","metadata":{"execution":{"iopub.status.busy":"2024-10-21T13:43:30.547006Z","iopub.execute_input":"2024-10-21T13:43:30.547346Z","iopub.status.idle":"2024-10-21T13:43:30.555458Z","shell.execute_reply.started":"2024-10-21T13:43:30.547313Z","shell.execute_reply":"2024-10-21T13:43:30.554469Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"},"metadata":{}}]},{"cell_type":"code","source":"x = train_data[:block_size]\ny = train_data[1:block_size+1]\nfor t in range(block_size):\n    context = x[:t+1]\n    target = y[t]\n    print(f\"when input is {context} the target: {target}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-21T13:43:30.556688Z","iopub.execute_input":"2024-10-21T13:43:30.557018Z","iopub.status.idle":"2024-10-21T13:43:30.566543Z","shell.execute_reply.started":"2024-10-21T13:43:30.556972Z","shell.execute_reply":"2024-10-21T13:43:30.565624Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"when input is tensor([18]) the target: 47\nwhen input is tensor([18, 47]) the target: 56\nwhen input is tensor([18, 47, 56]) the target: 57\nwhen input is tensor([18, 47, 56, 57]) the target: 58\nwhen input is tensor([18, 47, 56, 57, 58]) the target: 1\nwhen input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\nwhen input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\nwhen input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Batch size effectively utilizes the capability of GPUs","metadata":{}},{"cell_type":"code","source":"torch.manual_seed(1337)\nbatch_size = 4 # how many independent sequences will we process in parallel?\nblock_size = 8 # what is the maximum context length for predictions?\n\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix]) #Shifted by one element to the right\n    return x, y\n\nxb, yb = get_batch('train')\nprint('inputs:')\nprint(xb.shape)\nprint(xb)\nprint('targets:')\nprint(yb.shape)\nprint(yb)\n\nprint('----')\n\nfor b in range(batch_size): # batch dimension\n    for t in range(block_size): # time dimension\n        context = xb[b, :t+1]\n        target = yb[b,t]\n        print(f\"when input is {context.tolist()} the target: {target}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-21T13:43:30.567613Z","iopub.execute_input":"2024-10-21T13:43:30.567956Z","iopub.status.idle":"2024-10-21T13:43:30.602298Z","shell.execute_reply.started":"2024-10-21T13:43:30.567923Z","shell.execute_reply":"2024-10-21T13:43:30.601409Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"inputs:\ntorch.Size([4, 8])\ntensor([[24, 43, 58,  5, 57,  1, 46, 43],\n        [44, 53, 56,  1, 58, 46, 39, 58],\n        [52, 58,  1, 58, 46, 39, 58,  1],\n        [25, 17, 27, 10,  0, 21,  1, 54]])\ntargets:\ntorch.Size([4, 8])\ntensor([[43, 58,  5, 57,  1, 46, 43, 39],\n        [53, 56,  1, 58, 46, 39, 58,  1],\n        [58,  1, 58, 46, 39, 58,  1, 46],\n        [17, 27, 10,  0, 21,  1, 54, 39]])\n----\nwhen input is [24] the target: 43\nwhen input is [24, 43] the target: 58\nwhen input is [24, 43, 58] the target: 5\nwhen input is [24, 43, 58, 5] the target: 57\nwhen input is [24, 43, 58, 5, 57] the target: 1\nwhen input is [24, 43, 58, 5, 57, 1] the target: 46\nwhen input is [24, 43, 58, 5, 57, 1, 46] the target: 43\nwhen input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\nwhen input is [44] the target: 53\nwhen input is [44, 53] the target: 56\nwhen input is [44, 53, 56] the target: 1\nwhen input is [44, 53, 56, 1] the target: 58\nwhen input is [44, 53, 56, 1, 58] the target: 46\nwhen input is [44, 53, 56, 1, 58, 46] the target: 39\nwhen input is [44, 53, 56, 1, 58, 46, 39] the target: 58\nwhen input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\nwhen input is [52] the target: 58\nwhen input is [52, 58] the target: 1\nwhen input is [52, 58, 1] the target: 58\nwhen input is [52, 58, 1, 58] the target: 46\nwhen input is [52, 58, 1, 58, 46] the target: 39\nwhen input is [52, 58, 1, 58, 46, 39] the target: 58\nwhen input is [52, 58, 1, 58, 46, 39, 58] the target: 1\nwhen input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\nwhen input is [25] the target: 17\nwhen input is [25, 17] the target: 27\nwhen input is [25, 17, 27] the target: 10\nwhen input is [25, 17, 27, 10] the target: 0\nwhen input is [25, 17, 27, 10, 0] the target: 21\nwhen input is [25, 17, 27, 10, 0, 21] the target: 1\nwhen input is [25, 17, 27, 10, 0, 21, 1] the target: 54\nwhen input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n","output_type":"stream"}]},{"cell_type":"code","source":"print(xb)","metadata":{"execution":{"iopub.status.busy":"2024-10-21T13:43:30.603347Z","iopub.execute_input":"2024-10-21T13:43:30.603643Z","iopub.status.idle":"2024-10-21T13:43:30.608912Z","shell.execute_reply.started":"2024-10-21T13:43:30.603611Z","shell.execute_reply":"2024-10-21T13:43:30.607987Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n        [44, 53, 56,  1, 58, 46, 39, 58],\n        [52, 58,  1, 58, 46, 39, 58,  1],\n        [25, 17, 27, 10,  0, 21,  1, 54]])\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\n\nclass BigramLanguageModel(nn.Module):\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n        \n    def forward(self, idx, targets=None):\n        logits = self.token_embedding_table(idx)  # (B, T, C)\n        \n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)  # Reshape to (B*T, C)\n            targets = targets.view(B*T)  # Flatten targets\n            loss = F.cross_entropy(logits, targets)\n        \n        return logits, loss\n    \n    def generate(self, idx, max_new_tokens):\n        for _ in range(max_new_tokens):\n            # Obtain the predictions\n            logits, loss = self(idx)\n            # focus only on the last time step\n            logits = logits[:, -1, :] # becomes (B, C)\n            probs = F.softmax(logits, dim=-1) # (B, C)\n            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n        return idx\n\n    \n# Instantiate model\nm = BigramLanguageModel(vocab_size)\n\nlogits, loss = m(xb, yb)\n\n# Print shapes\nprint(\"Logits shape:\", logits.shape)  # Should be (B*T, C) -> (6, 10)\nprint(\"Loss:\", loss)  # Loss value\n\nprint(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))","metadata":{"execution":{"iopub.status.busy":"2024-10-21T13:43:30.610234Z","iopub.execute_input":"2024-10-21T13:43:30.610671Z","iopub.status.idle":"2024-10-21T13:43:30.722197Z","shell.execute_reply.started":"2024-10-21T13:43:30.610638Z","shell.execute_reply":"2024-10-21T13:43:30.721237Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Logits shape: torch.Size([32, 65])\nLoss: tensor(4.8786, grad_fn=<NllLossBackward0>)\n\nSr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n","output_type":"stream"}]},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-21T13:43:30.723362Z","iopub.execute_input":"2024-10-21T13:43:30.723735Z","iopub.status.idle":"2024-10-21T13:43:31.638959Z","shell.execute_reply.started":"2024-10-21T13:43:30.723702Z","shell.execute_reply":"2024-10-21T13:43:31.637970Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"batch_size = 32\nfor steps in range(10000): # increase number of steps for good results...\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = m(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\nprint(loss.item())\n","metadata":{"execution":{"iopub.status.busy":"2024-10-21T13:43:31.640295Z","iopub.execute_input":"2024-10-21T13:43:31.641274Z","iopub.status.idle":"2024-10-21T13:43:49.894724Z","shell.execute_reply.started":"2024-10-21T13:43:31.641226Z","shell.execute_reply":"2024-10-21T13:43:49.893760Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"2.5727508068084717\n","output_type":"stream"}]},{"cell_type":"code","source":"print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=300)[0].tolist()))","metadata":{"execution":{"iopub.status.busy":"2024-10-21T13:43:49.896137Z","iopub.execute_input":"2024-10-21T13:43:49.896566Z","iopub.status.idle":"2024-10-21T13:43:49.948299Z","shell.execute_reply.started":"2024-10-21T13:43:49.896520Z","shell.execute_reply":"2024-10-21T13:43:49.947321Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"\nIyoteng h hasbe pave pirance\nRie hicomyonthar's\nPlinseard ith henoure wounonthioneir thondy, y heltieiengerofo'dsssit ey\nKIN d pe wither vouprrouthercc.\nhathe; d!\nMy hind tt hinig t ouchos tes; st yo hind wotte grotonear 'so it t jod weancotha:\nh hay.JUCle n prids, r loncave w hollular s O:\nHIs; ht \n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Self-attention","metadata":{}},{"cell_type":"code","source":"torch.manual_seed(1337)\nB, T, C = 4, 8, 2\nx = torch.randn(B, T, C)\nx.shape","metadata":{"execution":{"iopub.status.busy":"2024-10-21T13:43:49.949479Z","iopub.execute_input":"2024-10-21T13:43:49.949807Z","iopub.status.idle":"2024-10-21T13:43:49.960026Z","shell.execute_reply.started":"2024-10-21T13:43:49.949774Z","shell.execute_reply":"2024-10-21T13:43:49.959057Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"torch.Size([4, 8, 2])"},"metadata":{}}]},{"cell_type":"code","source":"tril = torch.tril(torch.ones(T, T))\ntril","metadata":{"execution":{"iopub.status.busy":"2024-10-21T13:43:49.961374Z","iopub.execute_input":"2024-10-21T13:43:49.961778Z","iopub.status.idle":"2024-10-21T13:43:49.970421Z","shell.execute_reply.started":"2024-10-21T13:43:49.961727Z","shell.execute_reply":"2024-10-21T13:43:49.969477Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n        [1., 1., 0., 0., 0., 0., 0., 0.],\n        [1., 1., 1., 0., 0., 0., 0., 0.],\n        [1., 1., 1., 1., 0., 0., 0., 0.],\n        [1., 1., 1., 1., 1., 0., 0., 0.],\n        [1., 1., 1., 1., 1., 1., 0., 0.],\n        [1., 1., 1., 1., 1., 1., 1., 0.],\n        [1., 1., 1., 1., 1., 1., 1., 1.]])"},"metadata":{}}]},{"cell_type":"code","source":"wei = torch.zeros((T,T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei","metadata":{"execution":{"iopub.status.busy":"2024-10-21T13:43:49.972143Z","iopub.execute_input":"2024-10-21T13:43:49.972846Z","iopub.status.idle":"2024-10-21T13:43:49.979924Z","shell.execute_reply.started":"2024-10-21T13:43:49.972814Z","shell.execute_reply":"2024-10-21T13:43:49.979029Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n        [0., 0., 0., 0., 0., 0., -inf, -inf],\n        [0., 0., 0., 0., 0., 0., 0., -inf],\n        [0., 0., 0., 0., 0., 0., 0., 0.]])"},"metadata":{}}]},{"cell_type":"code","source":"wei = F.softmax(wei, dim=-1)\n\nwei","metadata":{"execution":{"iopub.status.busy":"2024-10-21T13:43:49.981436Z","iopub.execute_input":"2024-10-21T13:43:49.981807Z","iopub.status.idle":"2024-10-21T13:43:49.990267Z","shell.execute_reply.started":"2024-10-21T13:43:49.981773Z","shell.execute_reply":"2024-10-21T13:43:49.989458Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"},"metadata":{}}]},{"cell_type":"code","source":"xbow3 = wei @ x","metadata":{"execution":{"iopub.status.busy":"2024-10-21T13:43:49.991398Z","iopub.execute_input":"2024-10-21T13:43:49.992807Z","iopub.status.idle":"2024-10-21T13:43:50.002618Z","shell.execute_reply.started":"2024-10-21T13:43:49.992769Z","shell.execute_reply":"2024-10-21T13:43:50.001707Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# We want x[b,t] = mean_{i<=t} x[b,i]\nxbow = torch.zeros((B,T,C))\nfor b in range(B):\n    for t in range(T):\n        xprev = x[b,:t+1] # (t,C)\n        xbow[b,t] = torch.mean(xprev, 0)","metadata":{"execution":{"iopub.status.busy":"2024-10-21T13:43:50.003798Z","iopub.execute_input":"2024-10-21T13:43:50.004141Z","iopub.status.idle":"2024-10-21T13:43:50.010868Z","shell.execute_reply.started":"2024-10-21T13:43:50.004092Z","shell.execute_reply":"2024-10-21T13:43:50.009895Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"torch.allclose(xbow, xbow3)","metadata":{"execution":{"iopub.status.busy":"2024-10-21T13:43:50.012423Z","iopub.execute_input":"2024-10-21T13:43:50.012822Z","iopub.status.idle":"2024-10-21T13:43:50.020293Z","shell.execute_reply.started":"2024-10-21T13:43:50.012780Z","shell.execute_reply":"2024-10-21T13:43:50.019361Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"False"},"metadata":{}}]},{"cell_type":"code","source":"# version 4: self-attention!\ntorch.manual_seed(1337)\nB,T,C = 4,8,32 # batch, time, channels\nx = torch.randn(B,T,C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nvalue = nn.Linear(C, head_size, bias=False)\nk = key(x)   # (B, T, 16)\nq = query(x) # (B, T, 16)\nwei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n\ntril = torch.tril(torch.ones(T, T))\n#wei = torch.zeros((T,T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\n\nv = value(x)\nout = wei @ v\n#out = wei @ x\n\nout.shape","metadata":{"execution":{"iopub.status.busy":"2024-10-21T13:43:50.021655Z","iopub.execute_input":"2024-10-21T13:43:50.022000Z","iopub.status.idle":"2024-10-21T13:43:50.051716Z","shell.execute_reply.started":"2024-10-21T13:43:50.021962Z","shell.execute_reply":"2024-10-21T13:43:50.050897Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"torch.Size([4, 8, 16])"},"metadata":{}}]},{"cell_type":"code","source":"wei[0]","metadata":{"execution":{"iopub.status.busy":"2024-10-21T13:43:50.052976Z","iopub.execute_input":"2024-10-21T13:43:50.053266Z","iopub.status.idle":"2024-10-21T13:43:50.060186Z","shell.execute_reply.started":"2024-10-21T13:43:50.053234Z","shell.execute_reply":"2024-10-21T13:43:50.059317Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n       grad_fn=<SelectBackward0>)"},"metadata":{}}]},{"cell_type":"code","source":"out[0]","metadata":{"execution":{"iopub.status.busy":"2024-10-21T13:43:50.061536Z","iopub.execute_input":"2024-10-21T13:43:50.061882Z","iopub.status.idle":"2024-10-21T13:43:50.069864Z","shell.execute_reply.started":"2024-10-21T13:43:50.061846Z","shell.execute_reply":"2024-10-21T13:43:50.068751Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"tensor([[-0.1571,  0.8801,  0.1615, -0.7824, -0.1429,  0.7468,  0.1007, -0.5239,\n         -0.8873,  0.1907,  0.1762, -0.5943, -0.4812, -0.4860,  0.2862,  0.5710],\n        [ 0.6764, -0.5477, -0.2478,  0.3143, -0.1280, -0.2952, -0.4296, -0.1089,\n         -0.0493,  0.7268,  0.7130, -0.1164,  0.3266,  0.3431, -0.0710,  1.2716],\n        [ 0.4823, -0.1069, -0.4055,  0.1770,  0.1581, -0.1697,  0.0162,  0.0215,\n         -0.2490, -0.3773,  0.2787,  0.1629, -0.2895, -0.0676, -0.1416,  1.2194],\n        [ 0.1971,  0.2856, -0.1303, -0.2655,  0.0668,  0.1954,  0.0281, -0.2451,\n         -0.4647,  0.0693,  0.1528, -0.2032, -0.2479, -0.1621,  0.1947,  0.7678],\n        [ 0.2510,  0.7346,  0.5939,  0.2516,  0.2606,  0.7582,  0.5595,  0.3539,\n         -0.5934, -1.0807, -0.3111, -0.2781, -0.9054,  0.1318, -0.1382,  0.6371],\n        [ 0.3428,  0.4960,  0.4725,  0.3028,  0.1844,  0.5814,  0.3824,  0.2952,\n         -0.4897, -0.7705, -0.1172, -0.2541, -0.6892,  0.1979, -0.1513,  0.7666],\n        [ 0.1866, -0.0964, -0.1430,  0.3059,  0.0834, -0.0069, -0.2047, -0.1535,\n         -0.0762,  0.3269,  0.3090,  0.0766,  0.0992,  0.1656,  0.1975,  0.7625],\n        [ 0.1301, -0.0328, -0.4965,  0.2865,  0.2704, -0.2636, -0.0738,  0.3786,\n          0.0746,  0.0338,  0.0147,  0.3194,  0.2993, -0.1653, -0.0386,  0.3375]],\n       grad_fn=<SelectBackward0>)"},"metadata":{}}]},{"cell_type":"code","source":"k = torch.randn(B,T,head_size)\nq = torch.randn(B,T,head_size)\nwei = q @ k.transpose(-2, -1) * head_size**-0.5","metadata":{"execution":{"iopub.status.busy":"2024-10-21T13:43:50.075408Z","iopub.execute_input":"2024-10-21T13:43:50.075723Z","iopub.status.idle":"2024-10-21T13:43:50.082992Z","shell.execute_reply.started":"2024-10-21T13:43:50.075692Z","shell.execute_reply":"2024-10-21T13:43:50.082159Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"k.var()","metadata":{"execution":{"iopub.status.busy":"2024-10-21T13:43:50.084115Z","iopub.execute_input":"2024-10-21T13:43:50.084415Z","iopub.status.idle":"2024-10-21T13:43:50.090725Z","shell.execute_reply.started":"2024-10-21T13:43:50.084383Z","shell.execute_reply":"2024-10-21T13:43:50.089791Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"tensor(1.0449)"},"metadata":{}}]},{"cell_type":"code","source":"q.var()","metadata":{"execution":{"iopub.status.busy":"2024-10-21T13:43:50.091934Z","iopub.execute_input":"2024-10-21T13:43:50.092240Z","iopub.status.idle":"2024-10-21T13:43:50.099222Z","shell.execute_reply.started":"2024-10-21T13:43:50.092210Z","shell.execute_reply":"2024-10-21T13:43:50.098170Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"tensor(1.0700)"},"metadata":{}}]},{"cell_type":"code","source":"wei.var()","metadata":{"execution":{"iopub.status.busy":"2024-10-21T13:43:50.100430Z","iopub.execute_input":"2024-10-21T13:43:50.100772Z","iopub.status.idle":"2024-10-21T13:43:50.107123Z","shell.execute_reply.started":"2024-10-21T13:43:50.100741Z","shell.execute_reply":"2024-10-21T13:43:50.106235Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"tensor(1.0918)"},"metadata":{}}]},{"cell_type":"code","source":"torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)","metadata":{"execution":{"iopub.status.busy":"2024-10-21T13:43:50.108415Z","iopub.execute_input":"2024-10-21T13:43:50.108729Z","iopub.status.idle":"2024-10-21T13:43:50.116532Z","shell.execute_reply.started":"2024-10-21T13:43:50.108686Z","shell.execute_reply":"2024-10-21T13:43:50.115660Z"},"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"},"metadata":{}}]},{"cell_type":"code","source":"torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot","metadata":{"execution":{"iopub.status.busy":"2024-10-21T13:43:50.117631Z","iopub.execute_input":"2024-10-21T13:43:50.117954Z","iopub.status.idle":"2024-10-21T13:43:50.126703Z","shell.execute_reply.started":"2024-10-21T13:43:50.117923Z","shell.execute_reply":"2024-10-21T13:43:50.125837Z"},"trusted":true},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"},"metadata":{}}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\n# hyperparameters\nbatch_size = 16 # how many independent sequences will we process in parallel?\nblock_size = 32 # what is the maximum context length for predictions?\nmax_iters = 5000\neval_interval = 100\nlearning_rate = 1e-3\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\nn_embd = 64\nn_head = 4\nn_layer = 4\ndropout = 0.0\n# ------------\n\ntorch.manual_seed(1337)\n\n# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\n# create a mapping from characters to integers\nstoi = { ch:i for i,ch in enumerate(chars) }\nitos = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n\n# Train and test splits\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.9*len(data)) # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\n# data loading\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    x, y = x.to(device), y.to(device)\n    return x, y\n\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out\n\nclass Head(nn.Module):\n    \"\"\" one head of self-attention \"\"\"\n\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        B,T,C = x.shape\n        k = self.key(x)   # (B,T,C)\n        q = self.query(x) # (B,T,C)\n        # compute attention scores (\"affinities\")\n        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n        wei = F.softmax(wei, dim=-1) # (B, T, T)\n        wei = self.dropout(wei)\n        # perform the weighted aggregation of the values\n        v = self.value(x) # (B,T,C)\n        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n        return out\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\" multiple heads of self-attention in parallel \"\"\"\n\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        self.proj = nn.Linear(n_embd, n_embd)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.proj(out))\n        return out\n\nclass FeedFoward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.ReLU(),\n            nn.Linear(4 * n_embd, n_embd),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass Block(nn.Module):\n    \"\"\" Transformer block: communication followed by computation \"\"\"\n\n    def __init__(self, n_embd, n_head):\n        # n_embd: embedding dimension, n_head: the number of heads we'd like\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.ffwd = FeedFoward(n_embd)\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.ln2 = nn.LayerNorm(n_embd)\n\n    def forward(self, x):\n        x = x + self.sa(self.ln1(x))\n        x = x + self.ffwd(self.ln2(x))\n        return x\n\n# super simple bigram model\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n\n        # idx and targets are both (B,T) tensor of integers\n        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n        x = tok_emb + pos_emb # (B,T,C)\n        x = self.blocks(x) # (B,T,C)\n        x = self.ln_f(x) # (B,T,C)\n        logits = self.lm_head(x) # (B,T,vocab_size)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # crop idx to the last block_size tokens\n            idx_cond = idx[:, -block_size:]\n            # get the predictions\n            logits, loss = self(idx_cond)\n            # focus only on the last time step\n            logits = logits[:, -1, :] # becomes (B, C)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1) # (B, C)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n            # append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n        return idx\n\nmodel = BigramLanguageModel()\nm = model.to(device)\n# print the number of parameters in the model\nprint(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n\n# create a PyTorch optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\nfor iter in range(max_iters):\n\n    # every once in a while evaluate the loss on train and val sets\n    if iter % eval_interval == 0 or iter == max_iters - 1:\n        losses = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\n# generate from the model\ncontext = torch.zeros((1, 1), dtype=torch.long, device=device)\nprint(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n","metadata":{"execution":{"iopub.status.busy":"2024-10-21T13:43:50.127698Z","iopub.execute_input":"2024-10-21T13:43:50.128023Z","iopub.status.idle":"2024-10-21T13:47:56.628686Z","shell.execute_reply.started":"2024-10-21T13:43:50.127992Z","shell.execute_reply":"2024-10-21T13:47:56.627667Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"0.209729 M parameters\nstep 0: train loss 4.4116, val loss 4.4022\nstep 100: train loss 2.6568, val loss 2.6670\nstep 200: train loss 2.5090, val loss 2.5058\nstep 300: train loss 2.4194, val loss 2.4333\nstep 400: train loss 2.3500, val loss 2.3561\nstep 500: train loss 2.2967, val loss 2.3132\nstep 600: train loss 2.2412, val loss 2.2500\nstep 700: train loss 2.2046, val loss 2.2185\nstep 800: train loss 2.1639, val loss 2.1872\nstep 900: train loss 2.1244, val loss 2.1509\nstep 1000: train loss 2.1030, val loss 2.1300\nstep 1100: train loss 2.0701, val loss 2.1186\nstep 1200: train loss 2.0382, val loss 2.0804\nstep 1300: train loss 2.0248, val loss 2.0638\nstep 1400: train loss 1.9932, val loss 2.0369\nstep 1500: train loss 1.9702, val loss 2.0307\nstep 1600: train loss 1.9645, val loss 2.0496\nstep 1700: train loss 1.9403, val loss 2.0132\nstep 1800: train loss 1.9093, val loss 1.9961\nstep 1900: train loss 1.9080, val loss 1.9887\nstep 2000: train loss 1.8840, val loss 1.9942\nstep 2100: train loss 1.8707, val loss 1.9737\nstep 2200: train loss 1.8588, val loss 1.9622\nstep 2300: train loss 1.8569, val loss 1.9538\nstep 2400: train loss 1.8405, val loss 1.9415\nstep 2500: train loss 1.8163, val loss 1.9454\nstep 2600: train loss 1.8254, val loss 1.9393\nstep 2700: train loss 1.8115, val loss 1.9340\nstep 2800: train loss 1.8052, val loss 1.9246\nstep 2900: train loss 1.8053, val loss 1.9328\nstep 3000: train loss 1.7998, val loss 1.9247\nstep 3100: train loss 1.7707, val loss 1.9214\nstep 3200: train loss 1.7528, val loss 1.9135\nstep 3300: train loss 1.7558, val loss 1.9071\nstep 3400: train loss 1.7538, val loss 1.8961\nstep 3500: train loss 1.7392, val loss 1.8974\nstep 3600: train loss 1.7250, val loss 1.8891\nstep 3700: train loss 1.7297, val loss 1.8873\nstep 3800: train loss 1.7185, val loss 1.8923\nstep 3900: train loss 1.7212, val loss 1.8746\nstep 4000: train loss 1.7146, val loss 1.8628\nstep 4100: train loss 1.7149, val loss 1.8772\nstep 4200: train loss 1.7071, val loss 1.8665\nstep 4300: train loss 1.6992, val loss 1.8442\nstep 4400: train loss 1.7071, val loss 1.8711\nstep 4500: train loss 1.6914, val loss 1.8554\nstep 4600: train loss 1.6878, val loss 1.8389\nstep 4700: train loss 1.6838, val loss 1.8462\nstep 4800: train loss 1.6629, val loss 1.8425\nstep 4900: train loss 1.6694, val loss 1.8352\nstep 4999: train loss 1.6624, val loss 1.8233\n\nAnd they bride will to love the dishomber,\nOur and Our my call ands; and his us crown.\nWhre that a enswer, my fears'\nAnd I must own proof it.\nBut this nowle, at misters, I in latess, drevers, and the now our wantes like die; little;\nHone my sot, that speak ye will,\nHenry doessign swer:\nIs would that\nmost--so what evily we stilly rive\nAnd him his sounger-forgued kingn,\nTurtuffe ar is his shall do allood,\nThat Prive my of.\n\nHENRY BOLINGS:\nYour adsabe, I am tonguein courtear tey, rents\nInford carenk no tongue mary.\nYou come, my master and see-ennome.\n\nGLOUCESTER:\nYour must her thy weep on, with\nconfessyy sting might. \nCLARENCE:\nMy when soe everewber move made pass\nLet our grave to too stingd his refess, as not contle my sun?\nWhat, look too, moothy I want bear\nthat fraving with some. Pold;\nWhere doss well not no yow.\n\nPLLIFFORD:\nTheir him the not.\n\nPOLIXINy:\nFor hitoustius behorn, stre summe wips;\nGood what that the must perpless keep I so have you bade watch the\nought in noturself wellouge, I am with you,\nFor I huse no where Must\nAnd him sprive tan young: bay's bring!\n\nAMINFORD:\nAminds so tentless find, for I have Warm word,\nAnd a berth and bring aInish taltiess but maste be any in\nBecomes me at my breforeing brom not my have\neat wise antog you had'st He wheremper:\nIt lost. The oldrefest jurse: france,\nMad there'd heart sting it is steeme.\n\nPOMPEWOR:\nSlaw you your fsursed smy of vone,\nOn I have must a belingne;\nAnd him yong to may's friend, delely plains.\n\nISABERBY:\nAnd ull toeser, smetisor,\nHe use them wars, evis;\nRistrut of a-lates cornfut.\n\nPORANIUS:\nBy, for whome is my en wall prevens dest thy paten this.\n\nBOLINA:\nOur names no broth,\nGary as is it hitely, ah thine as is Prove.\nIs see that lay not, there wife\nOUT sup that hear I corcilt, of munge,\nAs lates here twreth with fair? But I have but the peerforfeling?\n\nCOMIOLUS:\nYou do the art; Saumines.\n\nCORIOLANUS:\nMay, make, wonse thou arbaturence stan,\nThrese parfort: I will down's for nother,\nAm I here here will\nMarr\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\n# hyperparameters\nbatch_size = 64 # how many independent sequences will we process in parallel?\nblock_size = 256 # what is the maximum context length for predictions?\nmax_iters = 2000\neval_interval = 500\nlearning_rate = 3e-4\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\nn_embd = 384\nn_head = 6\nn_layer = 6\ndropout = 0.2\n# ------------\n\ntorch.manual_seed(1337)\n\n# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n### Character-wise tokenization\n# All chars in input text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\n# create a mapping from characters to integers\nstoi = { ch:i for i,ch in enumerate(chars) }\nitos = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n\n# Train and test splits\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.9*len(data)) # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\n# data loading\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    x, y = x.to(device), y.to(device)\n    return x, y\n\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out\n\nclass Head(nn.Module):\n    \"\"\" one head of self-attention \"\"\"\n\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        # input of size (batch, time-step, channels)\n        # output of size (batch, time-step, head size)\n        B,T,C = x.shape\n        k = self.key(x)   # (B,T,hs)\n        q = self.query(x) # (B,T,hs)\n        # compute attention scores (\"affinities\")\n        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n        wei = F.softmax(wei, dim=-1) # (B, T, T)\n        wei = self.dropout(wei)\n        # perform the weighted aggregation of the values\n        v = self.value(x) # (B,T,hs)\n        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n        return out\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\" multiple heads of self-attention in parallel \"\"\"\n\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        self.proj = nn.Linear(head_size * num_heads, n_embd)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.proj(out))\n        return out\n\nclass FeedForward(nn.Module):\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.ReLU(),\n            nn.Linear(4 * n_embd, n_embd),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass Block(nn.Module):\n    \"\"\" Transformer block: communication followed by computation \"\"\"\n\n    def __init__(self, n_embd, n_head):\n        # n_embd: embedding dimension, n_head: the number of heads we'd like\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.ffwd = FeedForward(n_embd)\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.ln2 = nn.LayerNorm(n_embd)\n\n    def forward(self, x):\n        x = x + self.sa(self.ln1(x))\n        x = x + self.ffwd(self.ln2(x))\n        return x\n\nclass GPTLanguageModel(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n        # better init, not covered in the original GPT video, but important, will cover in followup video\n        self.apply(self.init_weights)\n\n    def init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n\n        # idx and targets are both (B,T) tensor of integers\n        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n        x = tok_emb + pos_emb # (B,T,C)\n        x = self.blocks(x) # (B,T,C)\n        x = self.ln_f(x) # (B,T,C)\n        logits = self.lm_head(x) # (B,T,vocab_size)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # crop idx to the last block_size tokens\n            idx_cond = idx[:, -block_size:]\n            # get the predictions\n            logits, loss = self(idx_cond)\n            # focus only on the last time step\n            logits = logits[:, -1, :] # becomes (B, C)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1) # (B, C)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n            # append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n        return idx\n\nmodel = GPTLanguageModel()\nm = model.to(device)\n# print the number of parameters in the model\nprint(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n\n# create a PyTorch optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\nfor iter in range(max_iters):\n\n    if iter % eval_interval == 0 or iter == max_iters - 1:\n        losses = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\n# generate from the model\ncontext = torch.zeros((1, 1), dtype=torch.long, device=device)\nprint(decode(m.generate(context, max_new_tokens=500)[0].tolist()))","metadata":{"execution":{"iopub.status.busy":"2024-10-21T13:47:56.629986Z","iopub.execute_input":"2024-10-21T13:47:56.630276Z","iopub.status.idle":"2024-10-21T14:00:11.171213Z","shell.execute_reply.started":"2024-10-21T13:47:56.630245Z","shell.execute_reply":"2024-10-21T14:00:11.170176Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"10.788929 M parameters\nstep 0: train loss 4.2221, val loss 4.2306\nstep 500: train loss 1.7569, val loss 1.9078\nstep 1000: train loss 1.3912, val loss 1.5980\nstep 1500: train loss 1.2682, val loss 1.5302\nstep 1999: train loss 1.1905, val loss 1.5147\n\n&C the father blessenece disdoy:\nThe love sisted, hereing, one of it!\n\nFirst Citizen:\nIr thou shalt, it be ere!\nWho we were safect, where indeck,\nUpon the procusion of conment, this noble sease\nI would not pleap to have burnt: For it is done;\nBut of that I have tand ala man; who think wellingly,\nSo every shall pose your entreation\nTo yield for it this heaven timely noice.\n\nHERMIONE:\nPray, intent of thou those wilt not the queen\nLainly forcy is fot pardone\nI that will as enverid the bearless of\nF\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport dropbox\nimport torch\n\n# Directory to temporarily save the model weights\nweights_dir = './weights/GPTModel/'\nif not os.path.exists(weights_dir):\n    os.makedirs(weights_dir)\n\n# Save the model weights locally\nmodel_save_path = f'{weights_dir}/gpt_model_weights.pth'\ntorch.save(model.state_dict(), model_save_path)\nprint(f\"Model weights saved locally to {model_save_path}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-21T14:00:11.172596Z","iopub.execute_input":"2024-10-21T14:00:11.173000Z","iopub.status.idle":"2024-10-21T14:00:11.275136Z","shell.execute_reply.started":"2024-10-21T14:00:11.172953Z","shell.execute_reply":"2024-10-21T14:00:11.274131Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"Model weights saved to ./weights/GPTModel//gpt_model_weights.pth\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install dropbox","metadata":{"execution":{"iopub.status.busy":"2024-10-21T14:01:02.002181Z","iopub.execute_input":"2024-10-21T14:01:02.002615Z","iopub.status.idle":"2024-10-21T14:01:15.815013Z","shell.execute_reply.started":"2024-10-21T14:01:02.002569Z","shell.execute_reply":"2024-10-21T14:01:15.813978Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"Collecting dropbox\n  Downloading dropbox-12.0.2-py3-none-any.whl.metadata (4.3 kB)\nRequirement already satisfied: requests>=2.16.2 in /opt/conda/lib/python3.10/site-packages (from dropbox) (2.32.3)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from dropbox) (1.16.0)\nCollecting stone<3.3.3,>=2 (from dropbox)\n  Downloading stone-3.3.1-py3-none-any.whl.metadata (8.0 kB)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.16.2->dropbox) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.16.2->dropbox) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.16.2->dropbox) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.16.2->dropbox) (2024.8.30)\nCollecting ply>=3.4 (from stone<3.3.3,>=2->dropbox)\n  Downloading ply-3.11-py2.py3-none-any.whl.metadata (844 bytes)\nDownloading dropbox-12.0.2-py3-none-any.whl (572 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m572.1/572.1 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading stone-3.3.1-py3-none-any.whl (162 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.3/162.3 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading ply-3.11-py2.py3-none-any.whl (49 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: ply, stone, dropbox\nSuccessfully installed dropbox-12.0.2 ply-3.11 stone-3.3.1\n","output_type":"stream"}]},{"cell_type":"code","source":"import dropbox\n# Dropbox setup\nDROPBOX_ACCESS_TOKEN = 'sl.B_P8a0riyBND_2P9ARUTB2SYMykcFXybmhOeC4GMhdEanW09d6xjObSgcDUISMNmo8JXkAFLwmi6IIcp50rcyVqxkIAzXGFez9f0OIV0lLfErUa7-YS55K3oTCTAoQvfwK_SUfpbWkyiUJQwuaYn-zA'  # Replace with your actual access token\ndbx = dropbox.Dropbox(DROPBOX_ACCESS_TOKEN)\n\n# Define Dropbox destination path\ndropbox_destination_path = f'/Apps/GPTModel/gpt_model_weights.pth'\n\n# Upload the file to Dropbox\nwith open(model_save_path, 'rb') as f:\n    dbx.files_upload(f.read(), dropbox_destination_path, mute=True)\n\nprint(f\"File uploaded successfully to Dropbox at '{dropbox_destination_path}'.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-21T14:01:15.817449Z","iopub.execute_input":"2024-10-21T14:01:15.817899Z","iopub.status.idle":"2024-10-21T14:01:19.377029Z","shell.execute_reply.started":"2024-10-21T14:01:15.817847Z","shell.execute_reply":"2024-10-21T14:01:19.376082Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"File uploaded successfully to Dropbox at '/Apps/GPTModel/gpt_model_weights.pth'.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}