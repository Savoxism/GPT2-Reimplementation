# GPT2-Reimplementation
This project aims to rebuild the GPT2 from scratch. It will give me the necessary engineering skills and intuition to pre-train a LM from scratch, not to mention refreshing some of the insights on the Transformer architecture. Due to limited computational resources, I pre-trained a much smaller model and consequently, underfitting occured. Nevertheless, I only desired the proof-of-concept.

If you wish to load the model, kindly do so in Huggingface: Savoxism/GPT2-Proof-of-Concept
